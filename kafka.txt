схема: 
{
    "$schema": "http://json-schema.org/draft-04/schema#",
    "type": "object",
    "properties": {
        "DATA": {
            "type": "object",
            "properties": {
                "tid": {
                    "type": "string"
                },
                "bin": {
                    "type": "string"
                },
                "date": {
                    "type": "string"
                },
                "event": {
                    "type": "string"
                }
            },
            "required": [
                "tid",
                "bin",
                "date"
            ]
        }
    },
    "required": [
        "DATA"
    ]
}    


Название события: ITSUPPORTPINGDATA
Имя потока: CYCPOS.ITSUPPORTPINGDATAEVENT.V1
Источник: CYCPOS
Домен: K2_IAZCommon
Форман схемы: JSON
Время хранения данных в потоке (минуты): 200
Максимальный размер пакета (Килобайты): 100
Пиковое значение TPS (запрос/секунда): 15
Пиковое значение TPS (запрос/сутки): 1 296 000
Количество партиций: 10

pip install confluent-kafka psycopg2-binary

import json
from confluent_kafka import Consumer, KafkaError, OFFSET_BEGINNING
from confluent_kafka.schema_registry import SchemaRegistryClient
from confluent_kafka.schema_registry.json_schema import JSONDeserializer
from threading import Thread
import psycopg2

# Конфигурация Kafka и Schema Registry
kafka_config = {
    'bootstrap.servers': 'your_kafka_broker',  # Укажите адрес вашего Kafka брокера
    'group.id': 'CYCPOS.ITSUPPORTPINGDATAEVENT.V1',  # Имя группы консумеров
    'auto.offset.reset': 'earliest',  # Начинать с самого начала, если офсетов еще нет
    'enable.auto.commit': False  # Отключаем авто-коммит офсетов
}

schema_registry_config = {
    'url': 'your_schema_registry_url'  # Укажите адрес вашего Schema Registry
}

topic = 'CYCPOS.ITSUPPORTPINGDATAEVENT.V1'

# Инициализация клиента Schema Registry
schema_registry_client = SchemaRegistryClient(schema_registry_config)

# Получение схемы из Schema Registry
schema_id = schema_registry_client.get_latest_version(f'{topic}-value').schema_id
schema = schema_registry_client.get_schema(schema_id).schema_str

# Десериализатор для JSON Schema
json_deserializer = JSONDeserializer(schema)

# Настройки подключения к базе данных PostgreSQL
db_config = {
    'dbname': 'your_dbname',
    'user': 'your_dbuser',
    'password': 'your_dbpassword',
    'host': 'your_dbhost',
    'port': 'your_dbport'
}

def save_to_db(data):
    try:
        connection = psycopg2.connect(**db_config)
        cursor = connection.cursor()

        insert_query = """
        INSERT INTO your_table_name (tid, bin, date, event)
        VALUES (%s, %s, %s, %s)
        """
        cursor.execute(insert_query, (data['DATA']['tid'], data['DATA']['bin'], data['DATA']['date'], data['DATA'].get('event')))

        connection.commit()
        cursor.close()
        connection.close()
    except Exception as e:
        print(f"Failed to save data to DB: {e}")

def consume_partition(partition):
    consumer = Consumer(kafka_config)

    # Назначаем конкретную партицию консумеру
    consumer.assign([{'topic': topic, 'partition': partition, 'offset': OFFSET_BEGINNING}])

    try:
        while True:
            msg = consumer.poll(1.0)  # Ожидание сообщения (1 секунда тайм-аут)

            if msg is None:
                continue
            if msg.error():
                if msg.error().code() == KafkaError._PARTITION_EOF:
                    continue
                else:
                    print(f'Error: {msg.error()}')
                    break

            # Получение и десериализация сообщения
            message_value = msg.value().decode('utf-8')
            try:
                data = json_deserializer(message_value, None)
                print(f'Partition {partition} - Received message: {data}')
                save_to_db(data)
                # Коммит офсета после успешной обработки сообщения
                consumer.commit()
            except Exception as e:
                print(f'Partition {partition} - Failed to deserialize or validate message: {e}')

    finally:
        # Закрытие консюмера при завершении
        consumer.close()


# Создание и запуск потоков для каждой партиции
threads = []
for i in range(10):
    thread = Thread(target=consume_partition, args=(i,))
    thread.start()
    threads.append(thread)

# Ожидание завершения всех потоков
for thread in threads:
    thread.join()



from kafka import KafkaConsumer
from kafka.errors import KafkaError
import logging
import base64
import json
import csv
import os

# Логирование
logging.basicConfig(level=logging.INFO)

# Конфигурация SSL для Kafka
ssl_certfile = '/home/bill_dev/test/cert/New_cert/chain_pem.pem'
ssl_pr = '/home/bill_dev/test/cert/New_cert/privkey.pem'
ssl_pb = '/home/bill_dev/test/cert/New_cert/new_published.pem'

# Конфигурация Kafka
group_id = 'Pos_Ping_Consumer'
topic = 'CYCPOS.ITSUPPORTPINGDATAEVENT'
servers = 'tkli-mvp1028.vm.mos.cloud.sbrf.ru:9093, tkli-mvp1029.vm.mos.cloud.sbrf.ru:9093, tkli-mvp1034.vm.mos.cloud.sbrf.ru:9093'

# Парсер данных PING_DATA
def parse_data(data):
    index = 0
    result = {}
    while index < len(data):
        tag_id = data[index]
        index += 1
        length = data[index]
        index += 1
        value = data[index:index + length]
        index += length
        result[tag_id] = value
    return result

# Интерпретатор тегов
def interpret_tags(decoder_str, ping_date):
    info = {
        'TAGM_IMEI': '',
        'TAGM_ICCID': '',
        'TAGM_TID': '',
        'TAGM_MODEL': '',
        'TAGM_SERIAL': '',
        'PING_DATE': ping_date  # Дата PING
    }
    
    for tag, value in decoder_str.items():
        if tag == 0xD1:
            info['TAGM_IMEI'] = value.decode('utf-8')
        elif tag == 0xD2:
            info['TAGM_ICCID'] = value.decode('utf-8')
        elif tag == 0xC1:
            info['TAGM_TID'] = value.decode('utf-8')
        elif tag == 0xC4:
            info['TAGM_MODEL'] = value.decode('utf-8')
        elif tag == 0xC5:
            info['TAGM_SERIAL'] = value.decode('utf-8')
    
    return info

# Функция для добавления данных в CSV
def append_to_csv(file_name, data):
    file_exists = os.path.isfile(file_name)
    with open(file_name, mode='a', newline='') as file:
        writer = csv.DictWriter(file, fieldnames=data.keys())
        if not file_exists:
            writer.writeheader()  # Записываем заголовки только один раз, если файла нет
        writer.writerow(data)

# Консумер Kafka
consumer = KafkaConsumer(
    topic,
    auto_offset_reset='earliest',
    api_version=(2, 5, 0),
    group_id=group_id,
    bootstrap_servers=servers.split(', '),
    enable_auto_commit=True,
    security_protocol='SSL',
    ssl_check_hostname=False,
    ssl_certfile=ssl_pb,
    ssl_cafile=ssl_certfile,
    ssl_keyfile=ssl_pr,
    max_poll_records=10,
    fetch_max_wait_ms=30 * 1000,
    max_poll_interval_ms=4601000,
    session_timeout_ms=60000,
    request_timeout_ms=6601000,
    connections_max_idle_ms=7601000,
    heartbeat_interval_ms=3000
)

try:
    logging.info("Начало чтения сообщений")
    for message in consumer:
        try:
            # Декодируем JSON-сообщение
            raw_data = message.value.decode('utf-8')
            logging.info(f"Получено сообщение: {raw_data}")
            
            # Предположим, что весь JSON — это список данных с несколькими пингами
            message_json = json.loads(raw_data)
            data_entries = message_json["DATA"]  # Достаем массив с данными
            
            # Обрабатываем каждый элемент массива отдельно
            for entry in data_entries:
                terminal_id = entry.get("TERMINAL_ID", "Неизвестный ID")
                ping_data_encoded = entry["PING_DATA"]
                ping_date = entry.get("PING_DATE", "Дата не указана")  # Получаем PING_DATE
                
                # Декодируем Base64
                decoded_bytes = base64.b64decode(ping_data_encoded)
                logging.info(f"Декодированные байты для TERMINAL_ID {terminal_id}: {decoded_bytes}")

                # Конвертация в hex, если это необходимо
                decoded_bytes_hex = bytes.fromhex(decoded_bytes.decode('ascii', errors='ignore'))
                
                # Парсинг данных
                ping_data = parse_data(decoded_bytes_hex)
                parsed_info = interpret_tags(ping_data, ping_date)  # Передаем PING_DATE

                # Записываем данные в CSV
                append_to_csv('ping_data.csv', parsed_info)
                logging.info(f"Данные записаны в CSV для TERMINAL_ID {terminal_id}")

        except Exception as e:
            logging.error(f"Ошибка обработки сообщения: {e}")

except KafkaError as e:
    logging.error(f"Ошибка Kafka: {e}")
except KeyboardInterrupt:
    logging.info("Остановка по запросу пользователя.")
finally:
    consumer.close()
    logging.info('Соединение с Kafka закрыто.')
______________________________________________________________________________________________

from kafka import KafkaConsumer
from kafka.errors import KafkaError
import logging
import base64
import json
import csv
import os

# Логирование
logging.basicConfig(level=logging.INFO)

# Конфигурация SSL для Kafka
ssl_certfile = '/home/bill_dev/test/cert/New_cert/chain_pem.pem'
ssl_pr = '/home/bill_dev/test/cert/New_cert/privkey.pem'
ssl_pb = '/home/bill_dev/test/cert/New_cert/new_published.pem'

# Конфигурация Kafka
group_id = 'Pos_Ping_Consumer'
topic = 'CYCPOS.ITSUPPORTPINGDATAEVENT'
servers = 'tkli-mvp1028.vm.mos.cloud.sbrf.ru:9093, tkli-mvp1029.vm.mos.cloud.sbrf.ru:9093, tkli-mvp1034.vm.mos.cloud.sbrf.ru:9093'

# Парсер данных PING_DATA
def parse_data(data):
    index = 0
    result = {}
    while index < len(data):
        tag_id = data[index]
        index += 1
        length = data[index]
        index += 1
        value = data[index:index + length]
        index += length
        result[tag_id] = value
    return result

# Интерпретатор тегов
def interpret_tags(decoder_str, ping_date):
    info = {
        'TAGM_IMEI': '',
        'TAGM_ICCID': '',
        'TAGM_TID': '',
        'TAGM_MODEL': '',
        'TAGM_SERIAL': '',
        'PING_DATE': ping_date  # Дата PING
    }
    
    for tag, value in decoder_str.items():
        if tag == 0xD1:
            info['TAGM_IMEI'] = value.decode('utf-8')
        elif tag == 0xD2:
            info['TAGM_ICCID'] = value.decode('utf-8')
        elif tag == 0xC1:
            info['TAGM_TID'] = value.decode('utf-8')
        elif tag == 0xC4:
            info['TAGM_MODEL'] = value.decode('utf-8')
        elif tag == 0xC5:
            info['TAGM_SERIAL'] = value.decode('utf-8')
    
    return info

# Функция для добавления данных в CSV
def append_to_csv(file_name, data):
    file_exists = os.path.isfile(file_name)
    with open(file_name, mode='a', newline='') as file:
        writer = csv.DictWriter(file, fieldnames=data.keys())
        if not file_exists:
            writer.writeheader()  # Записываем заголовки только один раз, если файла нет
        writer.writerow(data)

# Консумер Kafka
consumer = KafkaConsumer(
    topic,
    auto_offset_reset='earliest',
    api_version=(2, 5, 0),
    group_id=group_id,
    bootstrap_servers=servers.split(', '),
    enable_auto_commit=True,
    security_protocol='SSL',
    ssl_check_hostname=False,
    ssl_certfile=ssl_pb,
    ssl_cafile=ssl_certfile,
    ssl_keyfile=ssl_pr,
    max_poll_records=10,
    fetch_max_wait_ms=30 * 1000,
    max_poll_interval_ms=4601000,
    session_timeout_ms=60000,
    request_timeout_ms=6601000,
    connections_max_idle_ms=7601000,
    heartbeat_interval_ms=3000
)

try:
    logging.info("Начало чтения сообщений")
    for message in consumer:
        try:
            # Декодируем JSON-сообщение
            raw_data = message.value.decode('utf-8')
            logging.info(f"Получено сообщение: {raw_data}")
            
            # Проверяем, является ли сообщение валидным JSON
            try:
                message_json = json.loads(raw_data)
            except json.JSONDecodeError:
                logging.warning(f"Сообщение не является валидным JSON и будет проигнорировано: {raw_data}")
                continue  # Игнорируем сообщение

            # Проверяем наличие необходимых полей в сообщении
            if "DATA" not in message_json or not isinstance(message_json["DATA"], list):
                logging.warning(f"Сообщение не содержит корректное поле 'DATA' и будет проигнорировано: {raw_data}")
                continue  # Игнорируем сообщение
            
            data_entries = message_json["DATA"]  # Достаем массив с данными
            
            # Обрабатываем каждый элемент массива отдельно
            for entry in data_entries:
                if "TERMINAL_ID" not in entry or "PING_DATA" not in entry or "PING_DATE" not in entry:
                    logging.warning(f"Элемент не содержит необходимых полей и будет проигнорирован: {entry}")
                    continue  # Игнорируем элемент
                
                terminal_id = entry.get("TERMINAL_ID", "Неизвестный ID")
                ping_data_encoded = entry["PING_DATA"]
                ping_date = entry.get("PING_DATE", "Дата не указана")  # Получаем PING_DATE
                
                # Декодируем Base64
                try:
                    decoded_bytes = base64.b64decode(ping_data_encoded)
                except base64.binascii.Error:
                    logging.warning(f"Ошибка декодирования Base64 для TERMINAL_ID {terminal_id}. Сообщение проигнорировано.")
                    continue  # Игнорируем сообщение
                
                logging.info(f"Декодированные байты для TERMINAL_ID {terminal_id}: {decoded_bytes}")

                # Конвертация в hex, если это необходимо
                try:
                    decoded_bytes_hex = bytes.fromhex(decoded_bytes.decode('ascii', errors='ignore'))
                except ValueError:
                    logging.warning(f"Ошибка конвертации байтов в HEX для TERMINAL_ID {terminal_id}. Сообщение проигнорировано.")
                    continue  # Игнорируем сообщение
                
                # Парсинг данных
                ping_data = parse_data(decoded_bytes_hex)
                parsed_info = interpret_tags(ping_data, ping_date)  # Передаем PING_DATE

                # Записываем данные в CSV
                append_to_csv('ping_data.csv', parsed_info)
                logging.info(f"Данные записаны в CSV для TERMINAL_ID {terminal_id}")

        except Exception as e:
            logging.error(f"Ошибка обработки сообщения: {e}")

except KafkaError as e:
    logging.error(f"Ошибка Kafka: {e}")
except KeyboardInterrupt:
    logging.info("Остановка по запросу пользователя.")
finally:
    consumer.close()
    logging.info('Соединение с Kafka закрыто.')

