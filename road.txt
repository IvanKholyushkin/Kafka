!!!ВИТРИНЫ!!!!!

ПКАП: Три типа автоматизированных систем
Корпоративная аналитическая платформа (КАП) в Сбербанке включает три типа ПКАПов, каждая из которых предназначена для решения определённых задач. Эти системы помогают компании анализировать данные, разрабатывать модели и создавать отчёты. Рассмотрим каждую из них детально.
1. Аналитическая система (ПКАП АС)
Что это такое?
Аналитическая система — это автоматизированная система, которая обрабатывает и анализирует большие массивы данных. Это "мозговой центр", где данные анализируются и превращаются в полезные отчёты и выводы.
Основные задачи:
•	Расчёт витрин данных:
Витрина данных — это специально подготовленный набор данных для решения конкретных задач бизнеса. Например, для маркетинга можно создать витрину с данными о предпочтениях клиентов.
•	Исполнение моделей:
АС может запускать математические модели для прогнозирования (например, предсказание спроса на услуги или выявление рисков).
•	Потоковая обработка данных:
Это когда данные обрабатываются в режиме реального времени.
Для чего это нужно?
Аналитическая система позволяет банку оперативно обрабатывать большие объёмы данных и предоставлять отчёты, которые помогают принимать решения на основе фактов.
Согласование:
Создание такой АС требует тщательного планирования и согласования с архитекторами ИТ-систем в банке. Это гарантирует, что система будет работать корректно и безопасно.
________________________________________
2. Витрина данных (ПКАП ВД)
Что это такое?
Витрина данных — это автоматизированная система, которая создаёт наборы данных для решения конкретных задач. Например, для отдела продаж можно создать витрину с информацией о клиентах, интересующихся ипотекой.
Основные задачи:
•	Расчёт витрин данных:
Витрина — это подготовленный набор данных для анализа. Например, данные о транзакциях клиентов за последние шесть месяцев.
•	Исполнение моделей:
АС может запускать модели для прогнозирования будущих действий клиентов или оценки рисков.
•	Потоковая обработка данных:
Витрина данных также может обрабатывать данные в реальном времени.
Для чего это нужно?
Эта АС помогает бизнесу быстро получать данные для анализа, что сокращает время на подготовку отчётов и ускоряет принятие решений.
Согласование:
Создание витрины данных требует согласования с архитекторами ИТ-систем, чтобы обеспечить правильную структуру и безопасность данных.
________________________________________
3. Лаборатория данных (ПКАП ЛД)
Что это такое?
Лаборатория данных — это автоматизированная система для экспериментов с данными, разработки новых моделей и тестирования гипотез.
Основные задачи:
•	Ad hoc аналитика:
Возможность анализировать данные по мере необходимости. Например, если нужно быстро оценить результаты маркетинговой кампании, лаборатория данных помогает провести такой анализ.
•	Разработка прототипов витрин:
В лаборатории можно создавать тестовые витрины данных для проверки перед внедрением в промышленную эксплуатацию.
•	Разработка моделей:
Здесь разрабатываются и тестируются математические модели, которые затем могут быть внедрены в более крупные системы.
Для чего это нужно?
Лаборатория данных важна для исследовательских задач и экспериментов. Она позволяет специалистам работать с данными, разрабатывать новые подходы и проверять гипотезы.
Согласование:
Для создания лаборатории данных может не требоваться решение архитектурного совета, если используется общедоступная лаборатория. Однако для специфических проектов требуется согласование с архитекторами.
________________________________________
Когда использовать каждую из систем?
•	Аналитическая система (ПКАП АС) подойдёт, если нужно обрабатывать большие объёмы данных, получать отчёты и делать прогнозы на основе данных. Это основной инструмент для аналитиков и руководителей.
•	Витрина данных (ПКАП ВД) — это система для быстрой подготовки и обработки данных для конкретных задач бизнеса. Она нужна, когда нужно получить данные для анализа, например, для маркетинговой кампании или продаж.
•	Лаборатория данных (ПКАП ЛД) будет полезна, если нужно проводить исследования, тестировать новые гипотезы или разрабатывать новые модели. Это система для экспериментов и инноваций. ML
________________________________________
Таким образом, автоматизированные системы КАП решают свои задачи и помогают банку быстрее обрабатывать данные, создавать прогнозы и управлять бизнес-процессами.

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

!!!доклад + hadoop!!!

Доклад: Корпоративная Аналитическая Платформа (КАП) и использование Hadoop
Корпоративная аналитическая платформа (КАП) — это технологическая платформа банка, предназначенная для решения широкого спектра аналитических задач. Она играет ключевую роль в сборе, обработке и анализе данных, предоставляя бизнесу возможности для оперативного принятия решений на основе фактов и аналитики.
Компоненты КАП
КАП состоит из нескольких важных компонентов:
1.	Источники данных:
o	Включают внутренние и внешние источники, такие как CRM, ERP, интернет-магазины, внешние контрагенты и системы.
2.	Операционное хранилище данных (ODS):
o	Используется для хранения накопленных данных, которые могут не быть доступны в исходных источниках.
3.	Хранилище данных (DWH):
o	Ключевой элемент КАП, где происходит интеграция данных, их очистка, приведение к единой модели и последующая аналитическая обработка.
4.	Витрины данных:
o	Специализированные наборы данных для решения конкретных бизнес-задач. Витрины данных могут быть созданы для различных отделов и заказчиков банка.
5.	Инструменты ETL:
o	Выполняют извлечение, трансформацию и загрузку данных из источников в целевые системы. Важно отметить, что в КАП также используется ELT-подход, где трансформации данных происходят на стороне хранилища.
6.	Инструменты BI:
o	Предоставляют возможность визуализации данных через отчёты, диаграммы и дашборды. В банке используются Qlik Sense, SDP Analytics и OLAP Analytics на базе Kylin.
Технологии хранения и обработки данных
КАП активно использует решения на базе SDP Hadoop для хранения и обработки больших объёмов данных. Это решение обеспечивает высокую доступность, низкую стоимость хранения и возможность обработки как структурированных, так и неструктурированных данных.
•	SDP Hadoop поддерживает параллельную обработку данных и распределённые вычисления, что делает его идеальным для анализа больших массивов данных.
•	Также стоит отметить использование SberETL, который позволяет забирать данные из различных источников и проводить цепочку их преобразований. Интересен момент, что SberETL работает по принципу ELT, а не классического ETL, что повышает гибкость системы.
Преимущества использования Hadoop в КАП
Hadoop является основой для масштабируемого хранения данных и их обработки в банке. Основные его преимущества:
•	Низкая стоимость хранения данных.
•	Возможность работы с огромными объёмами как структурированных, так и неструктурированных данных.
•	Поддержка распределённых вычислений, что даёт возможность обрабатывать данные параллельно, снижая время выполнения аналитических запросов.
•	Высокая доступность системы, что позволяет выполнять задачи в реальном времени.
Вывод
КАП в Сбербанке — это важная часть аналитической инфраструктуры, обеспечивающая сбор, хранение и обработку данных для бизнеса. Использование таких технологий, как Hadoop, значительно упрощает управление большими объёмами данных, снижает стоимость и ускоряет аналитические процессы, что позволяет банку принимать более обоснованные и оперативные решения.

1. Почему Hadoop, а не традиционные базы данных?
Ответ:
Hadoop был разработан специально для работы с огромными объёмами данных, которые становятся слишком большими для традиционных реляционных баз данных. Вот несколько ключевых преимуществ Hadoop перед традиционными БД:
•	Масштабируемость: Hadoop может обрабатывать петабайты данных, добавляя узлы в кластер. Традиционные базы данных сталкиваются с ограничениями по масштабированию, когда объём данных резко возрастает.
•	Распределённое хранение и обработка: Данные в Hadoop распределяются по множеству узлов, что позволяет параллельно обрабатывать большие объёмы информации быстрее, чем в традиционных базах.
•	Обработка неструктурированных данных: Традиционные базы данных хорошо работают с таблицами, но плохо подходят для обработки неструктурированных данных (таких как текст, изображения или логи). Hadoop легко обрабатывает такие данные с помощью HDFS (Hadoop Distributed File System).
•	Низкая стоимость хранения: Хранение данных в Hadoop обходится дешевле, поскольку используется обычное железо, а не специализированное оборудование.
________________________________________
2. А можно ли обрабатывать данные традиционными базами данных в КАП?
Ответ:
Да, для некоторых задач традиционные базы данных всё ещё применяются в КАП. Например, для хранения критически важных транзакционных данных или данных, которые требуют высокой согласованности, мы можем использовать базы данных, такие как PostgreSQL. Однако для задач, связанных с анализом больших данных или неструктурированных данных, Hadoop предлагает более гибкие и масштабируемые решения.
________________________________________
3. Какие недостатки у Hadoop?
Ответ:
Hadoop имеет несколько недостатков, о которых стоит упомянуть:
•	Задержка: В отличие от баз данных реального времени, Hadoop менее эффективен для задач с низкой задержкой. Его основное назначение — это обработка больших данных пакетами.
•	Сложность: Hadoop сложнее в настройке и управлении по сравнению с традиционными базами данных, так как требует создания кластеров, настройки безопасности и мониторинга узлов.
•	Недостаточная транзакционная поддержка: Hadoop не поддерживает ACID-транзакции, как это делают реляционные базы данных, поэтому он не подходит для задач, требующих высокой консистентности данных.
________________________________________
4. Как Hadoop интегрируется в КАП?
Ответ:
Hadoop является неотъемлемой частью КАП для обработки больших объёмов данных, например, логов и телеметрии, поступающих в режиме реального времени. Он используется как для хранения данных, так и для их анализа с помощью таких инструментов, как Spark и Hive. Эти технологии помогают в проведении сложных аналитических задач и создают витрины данных для использования бизнесом.
________________________________________
5. Почему КАП так важна для бизнеса?
Ответ:
КАП — это ключевая инфраструктура, которая помогает банку обрабатывать и анализировать большие объёмы данных. Она обеспечивает:
•	Принятие решений на основе данных: КАП позволяет бизнес-подразделениям принимать обоснованные решения, анализируя данные в реальном времени.
•	Оптимизация бизнес-процессов: КАП автоматизирует обработку данных, что сокращает затраты и время на получение отчётов и аналитики.
•	Масштабируемость: КАП может адаптироваться под растущие потребности бизнеса, обрабатывая всё большие объёмы данных.
________________________________________
6. А можно ли использовать только традиционные базы данных в КАП?
Ответ:
В некоторых случаях традиционные базы данных используются в КАП, но только для ограниченных задач. Традиционные БД работают медленнее при обработке больших объёмов данных и не могут эффективно работать с неструктурированными данными. Hadoop и подобные решения добавляются в КАП для масштабируемой и быстрой обработки данных.
________________________________________
7. Как обеспечивается безопасность данных в Hadoop?
Ответ:
Hadoop обладает механизмами безопасности, которые включают:
•	Аутентификация: Использование Kerberos для контроля доступа.
•	Шифрование данных: Hadoop поддерживает шифрование данных как на уровне хранения, так и на уровне передачи.
•	Контроль доступа: В Hadoop можно настраивать права доступа на уровне файлов и каталогов в HDFS, что обеспечивает контроль над тем, кто и какие данные может просматривать или изменять.
________________________________________
8. Что делать, если Hadoop не подходит для конкретной задачи?
Ответ:
КАП предлагает гибкую архитектуру, которая поддерживает разные технологии для разных задач. Если Hadoop не подходит, мы можем использовать другие решения, такие как базы данных PostgreSQL для транзакционных задач, или NoSQL решения для хранения данных, которым не требуется строгая структура. Hadoop используется там, где требуются высокая масштабируемость и работа с большими объёмами данных.
________________________________________
9. Какие инструменты аналитики используются вместе с Hadoop в КАП?
Ответ:
В КАП используются такие инструменты, как:
•	Apache Spark: Для обработки данных в реальном времени и пакетного анализа.
•	Hive: Для SQL-запросов к большим данным в распределённой среде.
•	SberETL: Для извлечения, трансформации и загрузки данных в различные витрины и хранилища. Эти инструменты работают на базе Hadoop и позволяют быстро и эффективно анализировать большие объёмы данных.
________________________________________
10. Как Hadoop помогает в работе с неструктурированными данными?
Ответ:
Hadoop был разработан для работы с неструктурированными данными, такими как текстовые файлы, логи, изображения и видео. HDFS позволяет хранить такие данные без необходимости предварительно структурировать их, как это требуется в реляционных базах данных. Это делает Hadoop особенно эффективным для анализа неструктурированных данных в больших объёмах.
________________________________________
11. Можно ли использовать Spark без Hadoop?
Ответ:
Да, Spark может работать как автономно, так и в связке с Hadoop. Однако в контексте КАП и обработки больших данных Spark часто используется вместе с HDFS для хранения данных и MapReduce для распределённой обработки.
________________________________________
12. Какое оборудование требуется для Hadoop?
Ответ:
Hadoop спроектирован так, чтобы работать на стандартном серверном оборудовании. Это позволяет снизить стоимость хранения данных, поскольку можно использовать обычные серверы, а не дорогое специализированное оборудование, как в случае с традиционными базами данных.
________________________________________
Низкая стоимость хранения в Hadoop
Хранение данных в Hadoop действительно обходится дешевле по сравнению с традиционными решениями по нескольким причинам:
1.	Использование обычного оборудования (Commodity Hardware):
Hadoop специально разработан для работы на стандартном (обычном) серверном оборудовании. Это так называемое "commodity hardware", которое намного дешевле, чем специализированные серверы для традиционных баз данных, такие как Oracle Exadata или IBM Netezza. При использовании традиционных баз данных обычно требуются дорогие системы с высоким уровнем надёжности и производительности.
В случае с Hadoop, его распределённая архитектура позволяет использовать обычные сервера с возможностью замены или добавления узлов в случае поломки. Это значительно снижает затраты на инфраструктуру.
2.	Масштабируемость на уровне оборудования:
В традиционных системах база данных может расти до определённого предела, после которого необходимо модернизировать оборудование или полностью менять его на более мощное (вертикальное масштабирование). Hadoop же поддерживает горизонтальное масштабирование, что означает, что когда растут объёмы данных, просто добавляются новые узлы в кластер. Это позволяет увеличивать вычислительные и хранилищные мощности по мере необходимости, избегая огромных вложений в дорогое оборудование на старте.
3.	Меньшие требования к дисковым системам и резервированию:
В традиционных базах данных для обеспечения высокой доступности часто требуется RAID (Redundant Array of Independent Disks) или другие схемы резервирования данных на уровне оборудования. В Hadoop резервирование и восстановление данных организованы на уровне программного обеспечения через HDFS (Hadoop Distributed File System). HDFS сам дублирует данные на несколько узлов в кластере (обычно на три разных узла), что снижает необходимость использования дорогостоящих дисковых массивов и систем резервного копирования.
4.	Отказоустойчивость:
Благодаря своей распределённой архитектуре Hadoop сам по себе обеспечивает высокий уровень отказоустойчивости. В случае выхода из строя одного узла кластер автоматически перенаправляет задачи на другие узлы, что позволяет избежать простоев без необходимости использовать специализированное аппаратное обеспечение с высокими требованиями к надёжности.
5.	Открытое программное обеспечение:
Hadoop — это проект с открытым исходным кодом, поддерживаемый большим количеством разработчиков по всему миру. Это означает, что компании могут использовать Hadoop без необходимости платить за дорогостоящие лицензии, как это бывает с коммерческими системами управления базами данных (например, Oracle, Microsoft SQL Server и т.д.).
________________________________________
Пример экономии на практике:
Представим компанию, которая работает с петабайтами данных. В случае с традиционной базой данных для таких объёмов потребуются серьёзные затраты на оборудование и лицензии. Однако, развернув Hadoop на обычных серверах, компания может обрабатывать те же объёмы данных за значительно меньшую стоимость, при этом сохраняя гибкость в масштабировании и высокую отказоустойчивость.
________________________________________
Этот подход делает Hadoop популярным среди компаний, работающих с большими данными, такими как Google, Facebook, и многих других, поскольку позволяет сохранять огромные объёмы информации по относительно низкой стоимости.
Если требуется ещё больше деталей по этой теме, готов расширить!

Огромные данные (или Big Data) — это термин, который описывает массивы данных, которые слишком велики или сложны для обработки и анализа традиционными методами, такими как обычные реляционные базы данных. Важно не только то, что данных много, но и то, что они поступают быстро, из разных источников, и зачастую имеют сложную структуру.
Основные характеристики Big Data — это так называемые "3V":
1.	Volume (Объём):
Объёмы данных, с которыми работают компании, растут с каждым годом. Данные собираются из множества источников: социальные сети, сенсоры, мобильные устройства, видео, логи и многое другое. Это могут быть гигабайты, терабайты, петабайты, а иногда и эксабайты данных.
Пример:
Социальные сети, такие как Facebook или Twitter, ежедневно генерируют огромные объёмы данных от миллионов пользователей.
2.	Velocity (Скорость):
Данные поступают с огромной скоростью в реальном времени, и их нужно оперативно обрабатывать, чтобы извлечь полезную информацию. Например, компании должны обрабатывать потоки данных, поступающих от пользователей, сенсоров или интернет-трафика, практически мгновенно, чтобы предоставлять актуальные отчёты и аналитику.
Пример:
Транзакции в банке происходят каждую секунду, и для безопасности важно сразу выявлять подозрительные операции.
3.	Variety (Разнообразие):
Данные могут быть представлены в разных форматах: структурированные (таблицы и базы данных), полуструктурированные (XML, JSON) и неструктурированные (тексты, изображения, видео, аудиофайлы). Современные системы должны уметь работать со всеми этими типами данных.
Пример:
В интернет-магазине данные могут включать информацию о покупках (структурированные), отзывы пользователей (неструктурированные) и логи посещений сайта (полуструктурированные).
________________________________________
Дополнительные характеристики Big Data:
4.	Veracity (Достоверность):
Это показатель качества данных. В огромных объёмах данных может содержаться много "шума" — некорректных или неполных данных. Для анализа важно отделить полезную информацию от мусора.
Пример:
В социальных сетях люди могут оставлять ложные или ошибочные данные, которые могут исказить результаты анализа.
5.	Value (Ценность):
Сама по себе информация не имеет ценности, если её не анализировать и не превращать в полезные для бизнеса инсайты. Big Data позволяет извлекать ценную информацию для принятия обоснованных решений.
Пример:
Анализ данных о поведении клиентов может помочь компании предсказать спрос на продукцию и своевременно изменить предложение.

Почему Big Data важны?
Big Data предоставляет компаниям огромные возможности:
•	Персонализация услуг и продуктов: Анализ поведения клиентов помогает адаптировать предложения под индивидуальные потребности.
•	Улучшение бизнес-процессов: Системы анализа данных позволяют оптимизировать производство, логистику и другие процессы.
•	Машинное обучение и искусственный интеллект: Огромные объёмы данных необходимы для тренировки моделей машинного обучения, которые могут прогнозировать будущее поведение, анализировать риски и находить скрытые закономерности.
Традиционные базы данных и Big Data
Традиционные базы данных (например, реляционные базы данных) не предназначены для работы с Big Data, так как они:
•	Имеют ограничения по объёму хранимых данных.
•	Плохо справляются с неструктурированными данными (например, текстами или изображениями).
•	Могут замедляться при необходимости обработки данных в реальном времени.
Big Data требует таких технологий, как Hadoop, Spark и NoSQL-базы данных, которые позволяют эффективно хранить и обрабатывать большие объёмы данных с высокой скоростью.

Традиционные базы данных и Big Data
Традиционные базы данных (например, реляционные базы данных) не предназначены для работы с Big Data, так как они:
•	Имеют ограничения по объёму хранимых данных.
•	Плохо справляются с неструктурированными данными (например, текстами или изображениями).
•	Могут замедляться при необходимости обработки данных в реальном времени.
Big Data требует таких технологий, как Hadoop, Spark и NoSQL-базы данных, которые позволяют эффективно хранить и обрабатывать большие объёмы данных с высокой скоростью.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

!!!ROADMAP!!!!

1. Архитектура
•	На первом этапе был определен тип ПКАПа (выбор между витриной данных, аналитической системой или лабораторией данных). Этот этап завершен, и архитектура уже разработана.
2. Получение Dev стенда
•	Следующим шагом был заказ KTS (контрольно-технических средств) под SDP Hadoop. Это включает в себя заказ серверов для распределенной вычислительной платформы. Минимальная конфигурация включает 6 серверов с установкой JAVA 8 и соответствующими требованиями по CPU, RAM и HDD. Этот шаг уже завершен.
3. Заказ ТУЗ
•	На этом этапе происходит создание и согласование Технической Учетной Записи (ТУЗ) для доступа к необходимым системам и средам. Включает в себя согласование с информационной безопасностью и другими подразделениями.
4. Разработка витрины/ПКАПа
•	Этап разработки витрины данных, который уже был завершен, включает создание всех необходимых компонентов, настройку источников данных и подготовку системы к тестированию.
5. Тестирование витрины
•	На этом этапе осуществляется тестирование витрины для выявления возможных проблем и обеспечения корректности всех настроек и интеграций. Это важный шаг для подтверждения того, что данные корректно загружаются, обрабатываются и предоставляются на выходе.
6. Выбор ПРОМ стенда
•	На этом этапе осуществляется выбор стенда для развертывания в ПРОМе. Стенд должен быть готов для дальнейших этапов тестирования и развертывания.
7. Подготовка к выводу дистрибутива ПКАП ВД
•	Здесь происходит сбор полного комплекта документации, проверка релизного процесса, настройки командного бэклога и релизной политики.
8. Заказ развертывания дистрибутива
•	На этом этапе создается задача по заказу развертывания дистрибутива, после чего команда ПКАПа готовится к развертыванию системы в тестовой среде.
9. Прохождение ПСИ
•	ПСИ (Приемо-Сдаточные Испытания) – это обязательный этап, включающий тестирование системы с участием разных команд, включая безопасность, эксплуатацию и разработку.
10. Развертывание ПКАП в ПРОМе
•	Финальная установка ПКАП в ПРОМ среду, подготовка его к промышленной эксплуатации.
11. Опытная эксплуатация витрины/ПКАПа
•	Этот этап включает реальное использование витрины для обработки и анализа данных, но в контролируемом режиме. Все ошибки и улучшения фиксируются и исправляются до перевода в полную эксплуатацию.
12. Перевод в ПРОМ эксплуатацию
•	После опытной эксплуатации витрина полностью переводится в ПРОМ эксплуатацию для использования всеми подразделениями.
!!!!!!!13. Согласование TLA
•	Это согласование лицензий и соглашений, связанных с использованием ПКАПа в рамках эксплуатационной среды.
14. Шаблон для доступа к ПКАП в ДРУГе
•	Финальный этап – настройка и раздача доступа к системе для всех пользователей через соответствующие шаблоны и процедуры.


Ключевые отличия Лаборатории данных от других ПКАП:
Цель: ПКАП ЛД ориентирована на эксперименты и исследования, в то время как ПКАП АС больше предназначена для глубокого анализа данных и отчетности, а ПКАП ВД — для оперативной подготовки данных для бизнеса.

Гибкость: ПКАП ЛД предоставляет больше возможностей для экспериментов и разработки прототипов, тогда как другие системы ориентированы на строго определенные задачи


Работа с ML и AI: ПКАП ЛД — это основное место для разработки и обучения моделей машинного обучения и искусственного интеллекта, что делает ее незаменимой для команд, работающих в сфере инноваций и разработки интеллектуальных систем.

Цель и фокус ПКАП ВД:
Витрина данных (ПКАП ВД) создается в первую очередь для оперативной обработки и предоставления данных бизнесу. Основная задача ПКАП ВД — быстро подготовить данные для конкретных бизнес-задач

Витрины данных обычно используются для оперативной отчетности или поддержки решений в краткосрочной перспективе. Эти системы хорошо справляются с задачами, требующими быстрого доступа к данным, и предоставляют ответы на бизнес-вопросы, требующие немедленной реакции.


